{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/marin/Documents/Ma2/2024_C_AV_RPI-1/PythonTest\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "#add site-packages to the path\n",
    "\n",
    "#sys.path.append(\"/home/marin/.local/lib/python3.10/site-packages\")\n",
    "\n",
    "\n",
    "\n",
    "import csv \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "parent_dir = os.path.join(os.getcwd())\n",
    "print(parent_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m['gnss_data_new.csv', 'accelerometer_new.csv', 'barometer_new.csv', 'Wildhorn_Telemetry_CLEAN.csv', 'Wildhorn_Telemetry_RAW.csv']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# There are multiple folders in the parent directory. We need to get csv file each folder \n",
    "# there is an unknown number of levels till we reach the csv files.\n",
    "# do a recursive search for the csv files\n",
    "csvList = list()\n",
    "def get_files(extension ):\n",
    "\n",
    "    for root, dirs, files in os.walk(parent_dir):\n",
    "        for file in files:\n",
    "            \n",
    "            if file.endswith(extension):\n",
    "                csvList.append(os.path.join(root, file))\n",
    "    return csvList\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def get_first_row(path):\n",
    "    #print(f\"Reading the first row of the file: {path}\")\n",
    "    with open(path, 'r') as file:\n",
    "        data = file.read()\n",
    "    # Split the data into lines\n",
    "    lines = data.split('\\n')\n",
    "    var = lines[0].split(',')\n",
    "\n",
    "    if len(var) < 2:\n",
    "        var = lines[0].split(';')\n",
    "    return var\n",
    "\n",
    "\n",
    "def from_text_to_csv(path):\n",
    "    new_path = path.replace('.txt', '.csv')\n",
    "    last_var = path.split('/')[-1].replace('.txt', '')\n",
    "    with open(new_path, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Time', last_var])\n",
    "    \n",
    "    # Read the text file\n",
    "    with open(path, 'r') as file:\n",
    "        data = file.read()\n",
    "    # Split the data into lines\n",
    "    lines = data.split('\\n')\n",
    "   \n",
    "    # Create a progress bar\n",
    "    progress_bar = tqdm.tqdm(total=len(lines), desc='Converting lines')\n",
    "    \n",
    "    for line in lines:\n",
    "        elements = line.split(':')\n",
    "        last_elem = elements[-1]\n",
    "        tempLine = line.replace(last_elem, '')\n",
    "       \n",
    "        try:\n",
    "            date, time = tempLine.split(' ')\n",
    "        except ValueError:\n",
    "            # Handle the error here\n",
    "            print(\"Error occurred while splitting the line:\", line)\n",
    "            continue\n",
    "        date,time = tempLine.split(' ')\n",
    "        with open(new_path, 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([date, time, last_elem])\n",
    "\n",
    "        \n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "def var_Mapping():\n",
    "    text_files = get_files('.csv')\n",
    "    varMap = dict()\n",
    "    for file in text_files:\n",
    "        #add the file to the dictionary i\n",
    "        tempVar = get_first_row(file)\n",
    "        for i in tempVar:\n",
    "            if i not in varMap:\n",
    "                varMap[i] = [file]\n",
    "            else:\n",
    "                varMap[i].append(file)\n",
    "    #for i in varMap:\n",
    "    # make the variables green in color in the text\n",
    "        #print(f\"\\033[92m{i}\\033[0m :{varMap[i]} \")\n",
    "    return varMap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_norm(acceleration):\n",
    "    # Parse acceleration values from the CSV file\n",
    "    accel_values = acceleration.strip('(').strip(')').split(',')\n",
    "    accel_values = [value.strip().strip('(') for value in accel_values]\n",
    "\n",
    "    accel_values = [float(value) for value in accel_values]\n",
    "    \n",
    "    # Calculate the norm of the acceleration vector\n",
    "    norm = np.linalg.norm(accel_values)\n",
    "    return norm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_var(variables):\n",
    "    varMap = var_Mapping()\n",
    "    # Get the acceleration data from the CSV file\n",
    "    acceleration_file = varMap[variables]\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(acceleration_file)\n",
    "    # Calculate the norm of the acceleration vector\n",
    "    data['norm'] = data[variables].apply(calculate_norm)\n",
    "    # Plot the norm of the acceleration vector with row position on the x-axis\n",
    "    plt.plot(data['norm'])\n",
    "    plt.xlabel('Row position')\n",
    "    plt.ylabel(f'{variables}')\n",
    "    plt.title(f'{variables} vector over time')\n",
    "\n",
    "    #plt.show()\n",
    "    #save in folder -> \"plots\" with the name of the file being the name of the variables\n",
    "    if not os.path.exists('plots'):\n",
    "        os.makedirs('plots')\n",
    "    print(f\"Saving plot as {variables}.png\")\n",
    "    plt.savefig(f'plots/{variables}.png')\n",
    "    \n",
    "    # Create a new plot\n",
    "    fig = go.Figure()\n",
    "    # Add a line to the plot\n",
    "    fig.add_trace(go.Scatter(x=data.index, y=data['norm'], mode='lines', name=variables))\n",
    "    # Update the layout of the plot\n",
    "    fig.update_layout(title=f'{variables} vector over time', xaxis_title='Row position', yaxis_title=f'{variables} vector')\n",
    "    # Save the plot as an HTML file\n",
    "    fig.write_html(f'plots/{variables}.html')\n",
    "    # Show the plot\n",
    "\n",
    "#19289\n",
    "    \n",
    "def find_row_value(variables, row):\n",
    "    varMap = var_Mapping()\n",
    "    # Get the acceleration data from the CSV file\n",
    "    acceleration_file = varMap[variables]\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(acceleration_file)\n",
    "    # Calculate the norm of the acceleration vector\n",
    "    \n",
    "    return data['Time(ms)'][row]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plot_var('acceleration')\n",
    "\n",
    "def reevaluate_timeStamp(std_time, paths):\n",
    "\n",
    "    print (f\"Standard time: {std_time}\")\n",
    "    print(f\"Paths: {paths}\")\n",
    "\n",
    "\n",
    "    for path in paths:\n",
    "        print(f\"Reading the file: {path}\")\n",
    "        #read the csv file and change Time(s) to Time(ms) with panda\n",
    "        data = pd.read_csv(path)\n",
    "        #get the time column\n",
    "        time = data['Time(ms)']\n",
    "        #convert the time to ms\n",
    "        time_ms = time.apply(lambda x: from_shitStamp_to_ms(x))\n",
    "        #subtract the standard time from the time_ms\n",
    "\n",
    "        new_time = time_ms - std_time\n",
    "        #add the new time to the data\n",
    "        data['Time(ms)'] = new_time\n",
    "      \n",
    "        #save the new data to a new file\n",
    "        new_path = path.replace('.csv', '_new.csv')\n",
    "        data.to_csv(new_path, index=False)\n",
    "        print(f\"New file created: {new_path}\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def from_shitStamp_to_ms(shitStamp):\n",
    "    # Split the timestamp into its components\n",
    "    components = shitStamp.split(':')\n",
    "    # Remove the last empty string\n",
    "    components.pop()\n",
    " \n",
    "    # Convert the components to integers\n",
    "    components = [float(component) for component in components]\n",
    "    # Calculate the time in milliseconds\n",
    "    try:\n",
    "        time_ms = components[0] * 3600000 + components[1] * 60000 + components[2] * 1000\n",
    "        return int(time_ms)\n",
    "    except IndexError:\n",
    "        print(f\"Error occurred while converting the timestamp: {shitStamp},{components}\")\n",
    "\n",
    "\n",
    "\n",
    "def reavgnss_timeStamp(std_time, path):\n",
    "\n",
    "    #std_time is in s\n",
    "\n",
    "    std_time = std_time * 1000\n",
    "\n",
    "    #multiply by 1000 to get the time in ms all elements in column Time(s) using panda \n",
    "\n",
    "    #read the csv file\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    #get the time column\n",
    "    print(f\"Columns: {data.columns}\")\n",
    "    time = data['Time(s)']\n",
    "\n",
    "    #convert the time to ms\n",
    "    time_ms = time * 1000\n",
    "\n",
    "    #subtract the standard time from the time_ms\n",
    "    new_time = time_ms - std_time\n",
    "\n",
    "    #add the new time to the data\n",
    "    data['Time(ms)'] = new_time\n",
    "    #remove the old time column\n",
    "    data = data.drop(columns=['Time(s)'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #save the new data to a new file\n",
    "    new_path = path.replace('.csv', '_new.csv')\n",
    "    data.to_csv(new_path, index=False)\n",
    "    print(f\"New file created: {new_path}\")\n",
    "\n",
    "def mean_cycle(path):\n",
    "    #read the csv file\n",
    "    data = pd.read_csv(path)\n",
    "   \n",
    "    if 'Time(ms)' not in data.columns:\n",
    "        print(f\"Time(ms) not in {path}\")\n",
    "        return -1\n",
    "    #get the time column\n",
    "    time = data['Time(ms)']\n",
    "\n",
    "    #between each n and n+1, calculate the difference and add it to a list\n",
    "    time_0 = np.array(time)\n",
    "    time_1 = np.roll(time, 1)\n",
    "\n",
    "    #remove the first element of time_1 and the first element of time_0\n",
    "    time_1 = time_1[1:]\n",
    "    time_0 = time_0[1:]\n",
    "\n",
    "    time_diff = time_0 - time_1\n",
    "    print (f\"Time diff: {time_diff[:10]}\")\n",
    "    #calculate the mean of the list\n",
    "    mean_time_diff = np.mean(time_diff)\n",
    "    \n",
    "\n",
    "    return mean_time_diff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#from_shitStamp_to_ms('12:02:47.993748:')\n",
    "\n",
    "\n",
    "#43367993\n",
    "shitPaths = \"./barometer.csv\".split(';')\n",
    "\n",
    "#reevaluate_timeStamp(43367993, shitPaths)\n",
    "\n",
    "#reavgnss_timeStamp(2379.4135,\"/mnt/c/Users/marin/Documents/Ma2/2024_C_AV_RPI-1/PythonTest/ERT_Wildhorn_Flight_Data/Team12_PFR_Altitude_SRAD Avionics_Logging_Data_EuRoC2022/avionics_data/gnss_data.csv\")\n",
    "\n",
    "cycleMap = dict()\n",
    "print(f\"\\033[94m{list(map(lambda x: x.split('/')[-1],var_Mapping()['Time(ms)']))}\\033[0m\")\n",
    "#print (f\"{var_Mapping()['Time(ms)']}\")\n",
    "#for i in var_Mapping()['Time(ms)']:\n",
    "    #cycleMap[i] = mean_cycle(i)\n",
    "\n",
    "#print(f\"\\033[94m{cycleMap}\\033[0m\")\n",
    "\n",
    "def add_to_dic(key ,dic, value):\n",
    "    if key not in dic.keys():\n",
    "        dic[key] = [value]\n",
    "    else:\n",
    "        dic[key].append(value)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    progress_bar.close()\n",
    "    dataframeList = list_of_dataframes\n",
    "    \n",
    "    print(f\"Combined data: {combined_data.columns}\")\n",
    "    #save to a csv file\n",
    "    combined_data.to_csv('combined_data.csv', index=False)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths to combine: 3\n"
     ]
    }
   ],
   "source": [
    "paths_to_combine = [\"./accelerometer_new.csv\",\n",
    "                     \"./barometer_new.csv\",\n",
    "                    \"./gnss_data_new.csv\"]\n",
    "print (f\"Paths to combine: {len(paths_to_combine)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Time(ms)', 'acceleration'], dtype='object')\n",
      "Index(['Date', 'Time(ms)', 'pressure(Pa)'], dtype='object')\n",
      "Index(['altitude(m)', 'velocity(m/s)', 'Time(ms)'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "theBigones = []\n",
    "def seperateThedataframes(paths_to_combine):\n",
    "    for i in paths_to_combine:\n",
    "        theBigones.append(pd.read_csv(i))\n",
    "    return theBigones\n",
    "\n",
    "seperateThedataframes(paths_to_combine)\n",
    "for i in theBigones:\n",
    "    print(i.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_414038/316931837.py:1: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
      "  df_combined = theBigones[2].merge(theBigones[0], on='Time(ms)', how='outer').merge(theBigones[1], on='Time(ms)', how='outer')\n",
      "/tmp/ipykernel_414038/316931837.py:1: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
      "  df_combined = theBigones[2].merge(theBigones[0], on='Time(ms)', how='outer').merge(theBigones[1], on='Time(ms)', how='outer')\n",
      "/tmp/ipykernel_414038/316931837.py:4: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_combined = df_combined.fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_414038/316931837.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_combined = df_combined.fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data: Index(['altitude(m)', 'velocity(m/s)', 'Time(ms)', 'acceleration',\n",
      "       'pressure(Pa)'],\n",
      "      dtype='object')\n",
      "Combined data saved to combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "df_combined = theBigones[2].merge(theBigones[0], on='Time(ms)', how='outer').merge(theBigones[1], on='Time(ms)', how='outer')\n",
    "df_combined = df_combined.drop_duplicates(subset='Time(ms)', keep='first')\n",
    "df_combined = df_combined.sort_values(by='Time(ms)')\n",
    "df_combined = df_combined.fillna(method='ffill')\n",
    "df_combined = df_combined.fillna(method='bfill')\n",
    "\n",
    "#remove columns date_x and date_y\n",
    "df_combined = df_combined.drop(columns=['Date_x', 'Date_y'])\n",
    "\n",
    "print (f\"Combined data: {df_combined.columns}\")\n",
    "\n",
    "\n",
    "#save the file \n",
    "df_combined.to_csv('combined_data.csv', index=False)\n",
    "print(f\"Combined data saved to combined_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
